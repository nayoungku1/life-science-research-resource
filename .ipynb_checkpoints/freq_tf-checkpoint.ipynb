{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터셋 로드\n",
    "X = pd.read_csv(\"./data/train.csv\")\n",
    "y = pd.read_csv(\"./data/label_encoded_y.csv\")\n",
    "X_test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([\"ID\", \"SUBCLASS\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6201, 4384)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>ABCA4</th>\n",
       "      <th>ABCA5</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6197</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>T181S</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6201 rows × 4384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3 ABCA4 ABCA5  ... ZNF292  \\\n",
       "0        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "1        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "2     R895R   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "3        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "4        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "...     ...  ...   ...   ...  ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "6196     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6197     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6198     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6199     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6200     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "\n",
       "     ZNF365 ZNF639 ZNF707 ZNFX1 ZNRF4 ZPBP ZW10  ZWINT ZYX  \n",
       "0        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "1        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "2        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "3        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "4        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "...     ...    ...    ...   ...   ...  ...  ...    ...  ..  \n",
       "6196     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "6197     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "6198     WT     WT     WT    WT    WT   WT   WT  T181S  WT  \n",
       "6199     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "6200     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "\n",
       "[6201 rows x 4384 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WT', 'R895R', 'E1462K', 'K289T', 'I175F', 'D533D', 'L903L',\n",
       "       'S660R', 'P799S', 'L621P', 'M713I', 'A615A', 'R586*',\n",
       "       'Q1051H D152D', 'D638N', 'I984S', 'F352S', 'L1214L F572L',\n",
       "       'R1297R', 'N124N', 'I822I F229L', 'I481N', 'A1061V I1056I',\n",
       "       'S781S', 'L19P', 'E153K', 'T885A', 'F495L', 'M121I', 'D897H',\n",
       "       'H582H', 'L311P', 'D641E', 'S63F A79A', 'S1249F',\n",
       "       'E1311K E342K R147C E1311K E342K R147C', 'T411S', 'T1027T R719H',\n",
       "       'R360R', 'R853R', 'V207L', 'V206V', 'R732Q', 'C1321Y I848I P529H',\n",
       "       'V1255V V814V V551M', 'R174H', 'L670L', 'S136*', 'I175I', 'Y1104D',\n",
       "       'P92T', 'F1389F T1239T V129V L13I', 'A788A', 'S782C', 'A1108T',\n",
       "       'P380Q', 'S202S', 'L1067I', 'K1176M', 'Y1216Y', 'G852R', 'K1336K',\n",
       "       'I1091T', 'L1101L', 'S785P', 'Y1055C', 'S1210S', 'T1229S', 'G69E',\n",
       "       'Q1220H', 'V978V', 'E1137Q', 'P700A', 'S1102S S150S', 'F1028V',\n",
       "       'M121V', 'R1122H', 'L1008L', 'H156Y I175I', 'P36S', 'G1094V',\n",
       "       'P463P', 'V120V', 'N1038T', 'F365L', 'T728M', 'P157P',\n",
       "       'R732Q G280G S191Y', 'E737E', 'P969S E840*', 'G69R',\n",
       "       'L965R L461L E113*', 'D82D', 'L606L', 'K625K', 'S1071S', 'G367G',\n",
       "       'S276F', 'I1091V', 'T1285I', 'H446N', 'E184D', 'M1385T', 'A1043T',\n",
       "       'L578L', 'W1069* S710S', 'G706G T548I R174H', 'R1034S', 'S593S',\n",
       "       'V148V', 'N1413K', 'D1420N', 'K271N N159Y', 'F834F F155I', 'S351A',\n",
       "       'P1306Q', 'R1031Q', 'P759T', 'E518G', 'P1438P', 'L1324L', 'C48R',\n",
       "       'Q1425*', 'S571R', 'R147H', 'M1314I F1248L A176T', 'S525F',\n",
       "       'F219F', 'G1310G', 'D282N R147C', 'R147C', 'S525S', 'E1311K',\n",
       "       'F102F', 'W739*', 'L1324I', 'P1334Q', 'A1379A', 'P163S', 'D636N',\n",
       "       'S150S'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"A2M\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 함수 정의\n",
    "def mutation_count(value):\n",
    "    if pd.isna(value) or value == 'WT':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(value.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 셀의 값을 변환\n",
    "X_encoded = X.map(mutation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK2의 nan는 float형입니다.\n",
      "ATP6V1H의 nan는 float형입니다.\n",
      "CCRL2의 nan는 float형입니다.\n",
      "CFP의 nan는 float형입니다.\n",
      "CNOT2의 nan는 float형입니다.\n",
      "CRAT의 nan는 float형입니다.\n",
      "DPYSL4의 nan는 float형입니다.\n",
      "GUK1의 nan는 float형입니다.\n",
      "IER3의 nan는 float형입니다.\n",
      "INHBB의 nan는 float형입니다.\n",
      "KCNH1의 nan는 float형입니다.\n",
      "MYL1의 nan는 float형입니다.\n",
      "NDUFV1의 nan는 float형입니다.\n",
      "NUDT4의 nan는 float형입니다.\n",
      "POLD2의 nan는 float형입니다.\n",
      "PTCH1의 nan는 float형입니다.\n",
      "PTGES3의 nan는 float형입니다.\n",
      "RBM5의 nan는 float형입니다.\n",
      "SCAMP1의 nan는 float형입니다.\n",
      "SCNN1A의 nan는 float형입니다.\n",
      "SLC25A28의 nan는 float형입니다.\n",
      "SYBU의 nan는 float형입니다.\n",
      "TARS1의 nan는 float형입니다.\n",
      "TMEM97의 nan는 float형입니다.\n",
      "TNFAIP6의 nan는 float형입니다.\n"
     ]
    }
   ],
   "source": [
    "# 각 셀의 값이 float형인지 확인\n",
    "for column in X_test.columns:\n",
    "    for value in X_test[column]:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{column}의 {value}는 float형입니다.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WT', 'A36P', nan, 'K181N K173N K133N K139N', 'M237I',\n",
       "       'G205E G157E G197E G163E', 'I131I I125I I165I I173I',\n",
       "       'S208S S200S S160S S166S', 'I173I I125I I131I I165I',\n",
       "       'H143D H183D H149D H191D', 'P208P P216P P174P P168P', 'A22G A70G',\n",
       "       'R122G H145Q H105Q H111Q H153Q', 'T146T T194T T152T T186T',\n",
       "       'A4A A52A', 'G163E G197E G205E G157E', 'P208P P216P P168P P174P',\n",
       "       'A7G A55G', 'P174A P216A P208A P168A', 'Y193F Y201F Y159F Y153F',\n",
       "       'A36G', 'T47P T41P T89P', 'P23P'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[\"AK2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = X_test.map(mutation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, stratify=y, \n",
    "                                                  test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='macro_f1', **kwargs):\n",
    "        super(MacroF1, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.precision = self.add_weight(name='precision', initializer='zeros')\n",
    "        self.recall = self.add_weight(name='recall', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=1)\n",
    "        y_true = tf.argmax(y_true, axis=1)\n",
    "\n",
    "        # True positives, false positives, false negatives 계산\n",
    "        tp = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "        fp = tf.reduce_sum(tf.cast(tf.not_equal(y_true, y_pred), tf.float32))\n",
    "        fn = tf.reduce_sum(tf.cast(tf.not_equal(y_pred, y_true), tf.float32))\n",
    "\n",
    "        # precision, recall 계산\n",
    "        precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "        recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "        self.precision.assign_add(precision)\n",
    "        self.recall.assign_add(recall)\n",
    "\n",
    "    def result(self):\n",
    "        # F1-score 계산\n",
    "        f1 = 2 * (self.precision * self.recall) / (self.precision + self.recall + tf.keras.backend.epsilon())\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n",
    "        self.precision.assign(0)\n",
    "        self.recall.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = num_classes['SUBCLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 예시\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_train_onehot = np.squeeze(y_train_onehot)  # 불필요한 1차원을 제거하여 (32, 26)으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_onehot = np.eye(num_classes)[y_val]\n",
    "y_val_onehot = np.squeeze(y_val_onehot)  # 불필요한 1차원을 제거하여 (32, 26)으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241, 4384)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lsrr/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일 (macro F1-score 메트릭 사용)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[MacroF1()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.1689 - macro_f1: 6.2419\n",
      "Epoch 2/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6348 - macro_f1: 14.3675\n",
      "Epoch 3/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.0744 - macro_f1: 20.0945\n",
      "Epoch 4/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5824 - macro_f1: 24.1636\n",
      "Epoch 5/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2614 - macro_f1: 27.5941\n",
      "Epoch 6/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.0195 - macro_f1: 30.5131\n",
      "Epoch 7/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8215 - macro_f1: 32.1640\n",
      "Epoch 8/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6830 - macro_f1: 33.1893\n",
      "Epoch 9/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5780 - macro_f1: 33.8892\n",
      "Epoch 10/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4977 - macro_f1: 34.3335\n",
      "Epoch 11/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4354 - macro_f1: 34.5127\n",
      "Epoch 12/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3865 - macro_f1: 34.7397\n",
      "Epoch 13/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3477 - macro_f1: 35.0123\n",
      "Epoch 14/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3167 - macro_f1: 35.1179\n",
      "Epoch 15/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2916 - macro_f1: 35.1294\n",
      "Epoch 16/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2710 - macro_f1: 35.1600\n",
      "Epoch 17/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2541 - macro_f1: 35.2401\n",
      "Epoch 18/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2400 - macro_f1: 35.2182\n",
      "Epoch 19/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2281 - macro_f1: 35.1525\n",
      "Epoch 20/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2182 - macro_f1: 35.2148\n",
      "Epoch 21/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2098 - macro_f1: 35.2257\n",
      "Epoch 22/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2025 - macro_f1: 35.2008\n",
      "Epoch 23/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1962 - macro_f1: 35.1780\n",
      "Epoch 24/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1907 - macro_f1: 35.1786\n",
      "Epoch 25/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1859 - macro_f1: 35.1857\n",
      "Epoch 26/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1816 - macro_f1: 35.2623\n",
      "Epoch 27/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1778 - macro_f1: 35.2468\n",
      "Epoch 28/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1743 - macro_f1: 35.2377\n",
      "Epoch 29/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1713 - macro_f1: 35.2403\n",
      "Epoch 30/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1685 - macro_f1: 35.2193\n",
      "Epoch 31/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1661 - macro_f1: 35.2449\n",
      "Epoch 32/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1638 - macro_f1: 35.2449\n",
      "Epoch 33/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1617 - macro_f1: 35.2257\n",
      "Epoch 34/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1597 - macro_f1: 35.2668\n",
      "Epoch 35/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1580 - macro_f1: 35.2520\n",
      "Epoch 36/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1563 - macro_f1: 35.2704\n",
      "Epoch 37/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1548 - macro_f1: 35.2718\n",
      "Epoch 38/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1534 - macro_f1: 35.2718\n",
      "Epoch 39/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1521 - macro_f1: 35.2731\n",
      "Epoch 40/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1508 - macro_f1: 35.2702\n",
      "Epoch 41/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1496 - macro_f1: 35.2658\n",
      "Epoch 42/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1486 - macro_f1: 35.2710\n",
      "Epoch 43/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1475 - macro_f1: 35.2876\n",
      "Epoch 44/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1465 - macro_f1: 35.2832\n",
      "Epoch 45/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1456 - macro_f1: 35.2866\n",
      "Epoch 46/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1447 - macro_f1: 35.2866\n",
      "Epoch 47/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1438 - macro_f1: 35.2896\n",
      "Epoch 48/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1430 - macro_f1: 35.2670\n",
      "Epoch 49/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1423 - macro_f1: 35.2718\n",
      "Epoch 50/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1416 - macro_f1: 35.2698\n",
      "Epoch 51/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1409 - macro_f1: 35.2605\n",
      "Epoch 52/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1403 - macro_f1: 35.2605\n",
      "Epoch 53/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1397 - macro_f1: 35.2682\n",
      "Epoch 54/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1391 - macro_f1: 35.2737\n",
      "Epoch 55/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1385 - macro_f1: 35.2737\n",
      "Epoch 56/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1380 - macro_f1: 35.2814\n",
      "Epoch 57/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1375 - macro_f1: 35.2737\n",
      "Epoch 58/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1370 - macro_f1: 35.2830\n",
      "Epoch 59/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1365 - macro_f1: 35.2801\n",
      "Epoch 60/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1360 - macro_f1: 35.2801\n",
      "Epoch 61/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1356 - macro_f1: 35.2878\n",
      "Epoch 62/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1352 - macro_f1: 35.2737\n",
      "Epoch 63/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1347 - macro_f1: 35.2994\n",
      "Epoch 64/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1344 - macro_f1: 35.2959\n",
      "Epoch 65/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1340 - macro_f1: 35.3238\n",
      "Epoch 66/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1336 - macro_f1: 35.3228\n",
      "Epoch 67/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1332 - macro_f1: 35.3331\n",
      "Epoch 68/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1329 - macro_f1: 35.3352\n",
      "Epoch 69/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1326 - macro_f1: 35.3275\n",
      "Epoch 70/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1323 - macro_f1: 35.3376\n",
      "Epoch 71/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1319 - macro_f1: 35.3364\n",
      "Epoch 72/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1314 - macro_f1: 35.3388\n",
      "Epoch 73/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1312 - macro_f1: 35.3396\n",
      "Epoch 74/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1312 - macro_f1: 35.3279\n",
      "Epoch 75/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1309 - macro_f1: 35.3271\n",
      "Epoch 76/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1306 - macro_f1: 35.3196\n",
      "Epoch 77/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1303 - macro_f1: 35.3297\n",
      "Epoch 78/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1301 - macro_f1: 35.3188\n",
      "Epoch 79/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1298 - macro_f1: 35.3299\n",
      "Epoch 80/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1296 - macro_f1: 35.3299\n",
      "Epoch 81/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1294 - macro_f1: 35.3299\n",
      "Epoch 82/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1291 - macro_f1: 35.3299\n",
      "Epoch 83/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1289 - macro_f1: 35.3299\n",
      "Epoch 84/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1287 - macro_f1: 35.3299\n",
      "Epoch 85/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1285 - macro_f1: 35.3299\n",
      "Epoch 86/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1283 - macro_f1: 35.3167\n",
      "Epoch 87/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1280 - macro_f1: 35.3014\n",
      "Epoch 88/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1279 - macro_f1: 35.3254\n",
      "Epoch 89/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1277 - macro_f1: 35.3101\n",
      "Epoch 90/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1275 - macro_f1: 35.3297\n",
      "Epoch 91/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1273 - macro_f1: 35.3101\n",
      "Epoch 92/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1272 - macro_f1: 35.3261\n",
      "Epoch 93/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1270 - macro_f1: 35.3101\n",
      "Epoch 94/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1268 - macro_f1: 35.3121\n",
      "Epoch 95/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1266 - macro_f1: 35.3079\n",
      "Epoch 96/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1265 - macro_f1: 35.3036\n",
      "Epoch 97/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1263 - macro_f1: 35.2994\n",
      "Epoch 98/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1262 - macro_f1: 35.3153\n",
      "Epoch 99/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1260 - macro_f1: 35.2965\n",
      "Epoch 100/100\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1259 - macro_f1: 35.3123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d9531250>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_train, y_train_onehot, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "155/155 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.1664 - macro_f1: 69.4409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 856us/step\n",
      "검증 데이터 예측 결과: [21  8  2 ... 23 23 21]\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 예측\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# 예측 결과는 소프트맥스 확률로 나오므로, argmax로 각 클래스의 예측을 얻음\n",
    "y_val_pred_classes = tf.argmax(y_val_pred, axis=1)\n",
    "\n",
    "# 검증 결과 출력\n",
    "print(\"검증 데이터 예측 결과:\", y_val_pred_classes.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 675us/step - loss: 4.8281 - macro_f1: 6.1477\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터 성능 평가\n",
    "val_loss, val_f1 = model.evaluate(X_val, y_val_onehot)\n",
    "# print(f\"검증 데이터 - 손실: {val_loss}, F1-score: {val_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 550us/step\n",
      "테스트 데이터 예측 결과: [21 21  2 ...  8 10  6]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 예측\n",
    "y_test_pred = model.predict(X_test_encoded)\n",
    "\n",
    "# 테스트 데이터에서 각 클래스의 예측값을 argmax로 얻음\n",
    "y_test_pred_classes = tf.argmax(y_test_pred, axis=1)\n",
    "\n",
    "# 테스트 결과 출력\n",
    "print(\"테스트 데이터 예측 결과:\", y_test_pred_classes.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자에서 문자열로 매핑\n",
    "label_mapping = {\n",
    "    0: \"ACC\", 1: \"BLCA\", 2: \"BRCA\", 3: \"CESC\", 4: \"COAD\", 5: \"DLBC\", 6: \"GBMLGG\",\n",
    "    7: \"HNSC\", 8: \"KIPAN\", 9: \"KIRC\", 10: \"LAML\", 11: \"LGG\", 12: \"LIHC\",\n",
    "    13: \"LUAD\", 14: \"LUSC\", 15: \"OV\", 16: \"PAAD\", 17: \"PCPG\", 18: \"PRAD\",\n",
    "    19: \"SARC\", 20: \"SKCM\", 21: \"STES\", 22: \"TGCT\", 23: \"THCA\", 24: \"THYM\",\n",
    "    25: \"UCEC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test_pred (NumPy array) to a pandas Series\n",
    "y_test_pred_series = pd.Series(y_test_pred_classes)\n",
    "\n",
    "# Use map to replace the numeric labels with strings\n",
    "predictions_mapped = y_test_pred_series.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         STES\n",
       "1         STES\n",
       "2         BRCA\n",
       "3       GBMLGG\n",
       "4         LIHC\n",
       "         ...  \n",
       "2541      COAD\n",
       "2542     KIPAN\n",
       "2543     KIPAN\n",
       "2544      LAML\n",
       "2545    GBMLGG\n",
       "Length: 2546, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson = pd.read_csv(\"./data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>TEST_2541</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>TEST_2542</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>TEST_2543</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>TEST_2544</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>TEST_2545</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID SUBCLASS\n",
       "0     TEST_0000      LGG\n",
       "1     TEST_0001      LGG\n",
       "2     TEST_0002      LGG\n",
       "3     TEST_0003      LGG\n",
       "4     TEST_0004      LGG\n",
       "...         ...      ...\n",
       "2541  TEST_2541      LGG\n",
       "2542  TEST_2542      LGG\n",
       "2543  TEST_2543      LGG\n",
       "2544  TEST_2544      LGG\n",
       "2545  TEST_2545      LGG\n",
       "\n",
       "[2546 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson[\"SUBCLASS\"] = predictions_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.now()\n",
    "mmdd = today.strftime(\"%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson.to_csv(f'./submit/{mmdd}_epoch100_deep_learning_submission.csv', encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
