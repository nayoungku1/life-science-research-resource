{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터셋 로드\n",
    "X = pd.read_csv(\"./data/train.csv\")\n",
    "y = pd.read_csv(\"./data/label_encoded_y.csv\")\n",
    "X_test = pd.read_csv(\"./data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop([\"ID\", \"SUBCLASS\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6201, 4384)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2M</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>AADAT</th>\n",
       "      <th>AARS1</th>\n",
       "      <th>ABAT</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA2</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>ABCA4</th>\n",
       "      <th>ABCA5</th>\n",
       "      <th>...</th>\n",
       "      <th>ZNF292</th>\n",
       "      <th>ZNF365</th>\n",
       "      <th>ZNF639</th>\n",
       "      <th>ZNF707</th>\n",
       "      <th>ZNFX1</th>\n",
       "      <th>ZNRF4</th>\n",
       "      <th>ZPBP</th>\n",
       "      <th>ZW10</th>\n",
       "      <th>ZWINT</th>\n",
       "      <th>ZYX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R895R</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6196</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6197</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6198</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>T181S</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6199</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6200</th>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>...</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6201 rows × 4384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        A2M AAAS AADAT AARS1 ABAT ABCA1 ABCA2 ABCA3 ABCA4 ABCA5  ... ZNF292  \\\n",
       "0        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "1        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "2     R895R   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "3        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "4        WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "...     ...  ...   ...   ...  ...   ...   ...   ...   ...   ...  ...    ...   \n",
       "6196     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6197     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6198     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6199     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "6200     WT   WT    WT    WT   WT    WT    WT    WT    WT    WT  ...     WT   \n",
       "\n",
       "     ZNF365 ZNF639 ZNF707 ZNFX1 ZNRF4 ZPBP ZW10  ZWINT ZYX  \n",
       "0        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "1        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "2        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "3        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "4        WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "...     ...    ...    ...   ...   ...  ...  ...    ...  ..  \n",
       "6196     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "6197     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "6198     WT     WT     WT    WT    WT   WT   WT  T181S  WT  \n",
       "6199     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "6200     WT     WT     WT    WT    WT   WT   WT     WT  WT  \n",
       "\n",
       "[6201 rows x 4384 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WT', 'R895R', 'E1462K', 'K289T', 'I175F', 'D533D', 'L903L',\n",
       "       'S660R', 'P799S', 'L621P', 'M713I', 'A615A', 'R586*',\n",
       "       'Q1051H D152D', 'D638N', 'I984S', 'F352S', 'L1214L F572L',\n",
       "       'R1297R', 'N124N', 'I822I F229L', 'I481N', 'A1061V I1056I',\n",
       "       'S781S', 'L19P', 'E153K', 'T885A', 'F495L', 'M121I', 'D897H',\n",
       "       'H582H', 'L311P', 'D641E', 'S63F A79A', 'S1249F',\n",
       "       'E1311K E342K R147C E1311K E342K R147C', 'T411S', 'T1027T R719H',\n",
       "       'R360R', 'R853R', 'V207L', 'V206V', 'R732Q', 'C1321Y I848I P529H',\n",
       "       'V1255V V814V V551M', 'R174H', 'L670L', 'S136*', 'I175I', 'Y1104D',\n",
       "       'P92T', 'F1389F T1239T V129V L13I', 'A788A', 'S782C', 'A1108T',\n",
       "       'P380Q', 'S202S', 'L1067I', 'K1176M', 'Y1216Y', 'G852R', 'K1336K',\n",
       "       'I1091T', 'L1101L', 'S785P', 'Y1055C', 'S1210S', 'T1229S', 'G69E',\n",
       "       'Q1220H', 'V978V', 'E1137Q', 'P700A', 'S1102S S150S', 'F1028V',\n",
       "       'M121V', 'R1122H', 'L1008L', 'H156Y I175I', 'P36S', 'G1094V',\n",
       "       'P463P', 'V120V', 'N1038T', 'F365L', 'T728M', 'P157P',\n",
       "       'R732Q G280G S191Y', 'E737E', 'P969S E840*', 'G69R',\n",
       "       'L965R L461L E113*', 'D82D', 'L606L', 'K625K', 'S1071S', 'G367G',\n",
       "       'S276F', 'I1091V', 'T1285I', 'H446N', 'E184D', 'M1385T', 'A1043T',\n",
       "       'L578L', 'W1069* S710S', 'G706G T548I R174H', 'R1034S', 'S593S',\n",
       "       'V148V', 'N1413K', 'D1420N', 'K271N N159Y', 'F834F F155I', 'S351A',\n",
       "       'P1306Q', 'R1031Q', 'P759T', 'E518G', 'P1438P', 'L1324L', 'C48R',\n",
       "       'Q1425*', 'S571R', 'R147H', 'M1314I F1248L A176T', 'S525F',\n",
       "       'F219F', 'G1310G', 'D282N R147C', 'R147C', 'S525S', 'E1311K',\n",
       "       'F102F', 'W739*', 'L1324I', 'P1334Q', 'A1379A', 'P163S', 'D636N',\n",
       "       'S150S'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[\"A2M\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환 함수 정의\n",
    "def mutation_count(value):\n",
    "    if pd.isna(value) or value == 'WT':\n",
    "        return 0\n",
    "    else:\n",
    "        return len(value.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 셀의 값을 변환\n",
    "X_encoded = X.map(mutation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AK2의 nan는 float형입니다.\n",
      "ATP6V1H의 nan는 float형입니다.\n",
      "CCRL2의 nan는 float형입니다.\n",
      "CFP의 nan는 float형입니다.\n",
      "CNOT2의 nan는 float형입니다.\n",
      "CRAT의 nan는 float형입니다.\n",
      "DPYSL4의 nan는 float형입니다.\n",
      "GUK1의 nan는 float형입니다.\n",
      "IER3의 nan는 float형입니다.\n",
      "INHBB의 nan는 float형입니다.\n",
      "KCNH1의 nan는 float형입니다.\n",
      "MYL1의 nan는 float형입니다.\n",
      "NDUFV1의 nan는 float형입니다.\n",
      "NUDT4의 nan는 float형입니다.\n",
      "POLD2의 nan는 float형입니다.\n",
      "PTCH1의 nan는 float형입니다.\n",
      "PTGES3의 nan는 float형입니다.\n",
      "RBM5의 nan는 float형입니다.\n",
      "SCAMP1의 nan는 float형입니다.\n",
      "SCNN1A의 nan는 float형입니다.\n",
      "SLC25A28의 nan는 float형입니다.\n",
      "SYBU의 nan는 float형입니다.\n",
      "TARS1의 nan는 float형입니다.\n",
      "TMEM97의 nan는 float형입니다.\n",
      "TNFAIP6의 nan는 float형입니다.\n"
     ]
    }
   ],
   "source": [
    "# 각 셀의 값이 float형인지 확인\n",
    "for column in X_test.columns:\n",
    "    for value in X_test[column]:\n",
    "        if isinstance(value, float):\n",
    "            print(f\"{column}의 {value}는 float형입니다.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WT', 'A36P', nan, 'K181N K173N K133N K139N', 'M237I',\n",
       "       'G205E G157E G197E G163E', 'I131I I125I I165I I173I',\n",
       "       'S208S S200S S160S S166S', 'I173I I125I I131I I165I',\n",
       "       'H143D H183D H149D H191D', 'P208P P216P P174P P168P', 'A22G A70G',\n",
       "       'R122G H145Q H105Q H111Q H153Q', 'T146T T194T T152T T186T',\n",
       "       'A4A A52A', 'G163E G197E G205E G157E', 'P208P P216P P168P P174P',\n",
       "       'A7G A55G', 'P174A P216A P208A P168A', 'Y193F Y201F Y159F Y153F',\n",
       "       'A36G', 'T47P T41P T89P', 'P23P'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[\"AK2\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_encoded = X_test.map(mutation_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, stratify=y, \n",
    "                                                  test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MacroF1(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='macro_f1', **kwargs):\n",
    "        super(MacroF1, self).__init__(name=name, **kwargs)\n",
    "        self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
    "        self.false_positives = self.add_weight(name='fp', initializer='zeros')\n",
    "        self.false_negatives = self.add_weight(name='fn', initializer='zeros')\n",
    "        self.precision = self.add_weight(name='precision', initializer='zeros')\n",
    "        self.recall = self.add_weight(name='recall', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.argmax(y_pred, axis=1)\n",
    "        y_true = tf.argmax(y_true, axis=1)\n",
    "\n",
    "        # True positives, false positives, false negatives 계산\n",
    "        tp = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "        fp = tf.reduce_sum(tf.cast(tf.not_equal(y_true, y_pred), tf.float32))\n",
    "        fn = tf.reduce_sum(tf.cast(tf.not_equal(y_pred, y_true), tf.float32))\n",
    "\n",
    "        # precision, recall 계산\n",
    "        precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "        recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "\n",
    "        self.precision.assign_add(precision)\n",
    "        self.recall.assign_add(recall)\n",
    "\n",
    "    def result(self):\n",
    "        # F1-score 계산\n",
    "        f1 = 2 * (self.precision * self.recall) / (self.precision + self.recall + tf.keras.backend.epsilon())\n",
    "        return f1\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.true_positives.assign(0)\n",
    "        self.false_positives.assign(0)\n",
    "        self.false_negatives.assign(0)\n",
    "        self.precision.assign(0)\n",
    "        self.recall.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = y.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = num_classes['SUBCLASS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 예시\n",
    "y_train_onehot = np.eye(num_classes)[y_train]\n",
    "y_train_onehot = np.squeeze(y_train_onehot)  # 불필요한 1차원을 제거하여 (32, 26)으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_onehot = np.eye(num_classes)[y_val]\n",
    "y_val_onehot = np.squeeze(y_val_onehot)  # 불필요한 1차원을 제거하여 (32, 26)으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241, 4384)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/lsrr/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 모델 정의\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일 (macro F1-score 메트릭 사용)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[MacroF1()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1330 - macro_f1: 13.8878\n",
      "Epoch 2/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.3582 - macro_f1: 31.7137\n",
      "Epoch 3/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.6548 - macro_f1: 45.0306\n",
      "Epoch 4/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 1.2092 - macro_f1: 54.5623\n",
      "Epoch 5/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8891 - macro_f1: 61.1763\n",
      "Epoch 6/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6865 - macro_f1: 64.5699\n",
      "Epoch 7/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5805 - macro_f1: 66.3251\n",
      "Epoch 8/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4545 - macro_f1: 67.4976\n",
      "Epoch 9/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4391 - macro_f1: 67.8021\n",
      "Epoch 10/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3408 - macro_f1: 68.8281\n",
      "Epoch 11/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3042 - macro_f1: 68.8674\n",
      "Epoch 12/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2752 - macro_f1: 68.9700\n",
      "Epoch 13/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2530 - macro_f1: 69.1320\n",
      "Epoch 14/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.2359 - macro_f1: 69.1510\n",
      "Epoch 15/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2221 - macro_f1: 69.2624\n",
      "Epoch 16/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2110 - macro_f1: 69.2320\n",
      "Epoch 17/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2020 - macro_f1: 69.2208\n",
      "Epoch 18/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1943 - macro_f1: 69.1613\n",
      "Epoch 19/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1878 - macro_f1: 69.1236\n",
      "Epoch 20/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1824 - macro_f1: 69.1965\n",
      "Epoch 21/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1776 - macro_f1: 69.2925\n",
      "Epoch 22/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.1734 - macro_f1: 69.3231\n",
      "Epoch 23/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1697 - macro_f1: 69.2881\n",
      "Epoch 24/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1665 - macro_f1: 69.2939\n",
      "Epoch 25/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1636 - macro_f1: 69.3636\n",
      "Epoch 26/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1609 - macro_f1: 69.5000\n",
      "Epoch 27/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1585 - macro_f1: 69.5142\n",
      "Epoch 28/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1564 - macro_f1: 69.5423\n",
      "Epoch 29/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1544 - macro_f1: 69.5335\n",
      "Epoch 30/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1526 - macro_f1: 69.6206\n",
      "Epoch 31/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1509 - macro_f1: 69.6240\n",
      "Epoch 32/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1494 - macro_f1: 69.7280\n",
      "Epoch 33/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1479 - macro_f1: 69.7051\n",
      "Epoch 34/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1465 - macro_f1: 69.8029\n",
      "Epoch 35/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1453 - macro_f1: 69.8109\n",
      "Epoch 36/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1442 - macro_f1: 69.7977\n",
      "Epoch 37/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1430 - macro_f1: 69.7881\n",
      "Epoch 38/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1421 - macro_f1: 69.8225\n",
      "Epoch 39/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1414 - macro_f1: 69.8397\n",
      "Epoch 40/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1404 - macro_f1: 69.9639\n",
      "Epoch 41/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1396 - macro_f1: 69.9675\n",
      "Epoch 42/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1388 - macro_f1: 69.9617\n",
      "Epoch 43/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1380 - macro_f1: 69.9275\n",
      "Epoch 44/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1373 - macro_f1: 69.9818\n",
      "Epoch 45/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1366 - macro_f1: 69.9992\n",
      "Epoch 46/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1359 - macro_f1: 70.0100\n",
      "Epoch 47/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1354 - macro_f1: 70.0543\n",
      "Epoch 48/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1349 - macro_f1: 70.0545\n",
      "Epoch 49/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1344 - macro_f1: 69.9870\n",
      "Epoch 50/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1340 - macro_f1: 69.9768\n",
      "Epoch 51/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1335 - macro_f1: 70.0232\n",
      "Epoch 52/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1330 - macro_f1: 70.1346\n",
      "Epoch 53/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1325 - macro_f1: 70.2238\n",
      "Epoch 54/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1321 - macro_f1: 70.1550\n",
      "Epoch 55/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1316 - macro_f1: 70.0703\n",
      "Epoch 56/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1313 - macro_f1: 69.9920\n",
      "Epoch 57/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1310 - macro_f1: 70.0709\n",
      "Epoch 58/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1305 - macro_f1: 70.0599\n",
      "Epoch 59/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.1302 - macro_f1: 70.0341\n",
      "Epoch 60/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.1298 - macro_f1: 70.1098\n",
      "Epoch 61/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1295 - macro_f1: 70.0833\n",
      "Epoch 62/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1291 - macro_f1: 69.9780\n",
      "Epoch 63/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1289 - macro_f1: 69.9159\n",
      "Epoch 64/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1286 - macro_f1: 69.8446\n",
      "Epoch 65/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1282 - macro_f1: 69.7875\n",
      "Epoch 66/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1280 - macro_f1: 69.7310\n",
      "Epoch 67/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1277 - macro_f1: 69.7298\n",
      "Epoch 68/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1275 - macro_f1: 69.7398\n",
      "Epoch 69/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1271 - macro_f1: 69.8598\n",
      "Epoch 70/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1268 - macro_f1: 69.8624\n",
      "Epoch 71/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1267 - macro_f1: 69.6811\n",
      "Epoch 72/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1264 - macro_f1: 69.7586\n",
      "Epoch 73/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1261 - macro_f1: 69.7851\n",
      "Epoch 74/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1260 - macro_f1: 69.7971\n",
      "Epoch 75/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1743 - macro_f1: 69.3203\n",
      "Epoch 76/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2446 - macro_f1: 70.5208\n",
      "Epoch 77/77\n",
      "\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1297 - macro_f1: 70.9083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2d7b7e390>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_train, y_train_onehot, epochs=77, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "155/155 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - loss: 0.1664 - macro_f1: 69.4409"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 787us/step\n",
      "검증 데이터 예측 결과: [21  8  2 ... 23 23 12]\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터에 대한 예측\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "# 예측 결과는 소프트맥스 확률로 나오므로, argmax로 각 클래스의 예측을 얻음\n",
    "y_val_pred_classes = tf.argmax(y_val_pred, axis=1)\n",
    "\n",
    "# 검증 결과 출력\n",
    "print(\"검증 데이터 예측 결과:\", y_val_pred_classes.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 970us/step - loss: 5.2823 - macro_f1: 5.8269\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터 성능 평가\n",
    "val_loss, val_f1 = model.evaluate(X_val, y_val_onehot)\n",
    "# print(f\"검증 데이터 - 손실: {val_loss}, F1-score: {val_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m80/80\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 545us/step\n",
      "테스트 데이터 예측 결과: [21  3  2 ...  3 10  6]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에 대한 예측\n",
    "y_test_pred = model.predict(X_test_encoded)\n",
    "\n",
    "# 테스트 데이터에서 각 클래스의 예측값을 argmax로 얻음\n",
    "y_test_pred_classes = tf.argmax(y_test_pred, axis=1)\n",
    "\n",
    "# 테스트 결과 출력\n",
    "print(\"테스트 데이터 예측 결과:\", y_test_pred_classes.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자에서 문자열로 매핑\n",
    "label_mapping = {\n",
    "    0: \"ACC\", 1: \"BLCA\", 2: \"BRCA\", 3: \"CESC\", 4: \"COAD\", 5: \"DLBC\", 6: \"GBMLGG\",\n",
    "    7: \"HNSC\", 8: \"KIPAN\", 9: \"KIRC\", 10: \"LAML\", 11: \"LGG\", 12: \"LIHC\",\n",
    "    13: \"LUAD\", 14: \"LUSC\", 15: \"OV\", 16: \"PAAD\", 17: \"PCPG\", 18: \"PRAD\",\n",
    "    19: \"SARC\", 20: \"SKCM\", 21: \"STES\", 22: \"TGCT\", 23: \"THCA\", 24: \"THYM\",\n",
    "    25: \"UCEC\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_test_pred (NumPy array) to a pandas Series\n",
    "y_test_pred_series = pd.Series(y_test_pred_classes)\n",
    "\n",
    "# Use map to replace the numeric labels with strings\n",
    "predictions_mapped = y_test_pred_series.map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         STES\n",
       "1         CESC\n",
       "2         BRCA\n",
       "3       GBMLGG\n",
       "4         UCEC\n",
       "         ...  \n",
       "2541      COAD\n",
       "2542      KIRC\n",
       "2543      CESC\n",
       "2544      LAML\n",
       "2545    GBMLGG\n",
       "Length: 2546, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson = pd.read_csv(\"./data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>SUBCLASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>TEST_2541</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>TEST_2542</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2543</th>\n",
       "      <td>TEST_2543</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2544</th>\n",
       "      <td>TEST_2544</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2545</th>\n",
       "      <td>TEST_2545</td>\n",
       "      <td>LGG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2546 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID SUBCLASS\n",
       "0     TEST_0000      LGG\n",
       "1     TEST_0001      LGG\n",
       "2     TEST_0002      LGG\n",
       "3     TEST_0003      LGG\n",
       "4     TEST_0004      LGG\n",
       "...         ...      ...\n",
       "2541  TEST_2541      LGG\n",
       "2542  TEST_2542      LGG\n",
       "2543  TEST_2543      LGG\n",
       "2544  TEST_2544      LGG\n",
       "2545  TEST_2545      LGG\n",
       "\n",
       "[2546 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson[\"SUBCLASS\"] = predictions_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "today = datetime.now()\n",
    "mmdd = today.strftime(\"%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "submisson.to_csv(f'./submit/{mmdd}_epoch100_deep_learning_submission.csv', encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
